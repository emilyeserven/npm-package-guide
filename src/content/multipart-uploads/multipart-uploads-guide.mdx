---
id: "multipart-uploads-guide"
title: "Multipart Uploads üì§"
guide: "multipart-uploads"
linkRefs:
  - id: "rfc-2046"
    note: "The RFC defining multipart content types"
  - id: "mdn-formdata"
    note: "MDN reference for the FormData API"
  - id: "mdn-fetch"
    note: "MDN reference for the Fetch API"
  - id: "npm-multer"
    note: "Popular Node.js multipart parser"
  - id: "npm-busboy"
    note: "Streaming multipart parser for Node.js"
  - id: "tus-protocol"
    note: "Open protocol for resumable uploads"
  - id: "npm-uppy"
    note: "Modular file upload suite"
---

<SectionTitle>{frontmatter.title}</SectionTitle>

<Toc>
  <TocLink id="toc-what">What is multipart/form-data?</TocLink>
  <TocLink id="toc-anatomy">Anatomy of a Request</TocLink>
  <TocLink id="toc-pipeline">The Upload Pipeline</TocLink>
  <TocLink id="toc-implementation">Implementation</TocLink>
  <TocLink id="toc-demo">Live Demo</TocLink>
  <TocLink id="toc-when">When to Use Multipart</TocLink>
  <TocLink id="toc-packages">Vetting Upload Packages</TocLink>
  <TocLink id="toc-gotchas">Common Gotchas</TocLink>
</Toc>

<SectionIntro>
How files actually travel from a browser to a server ‚Äî the encoding, the pipeline,
the pitfalls, and what to look for in upload libraries.
</SectionIntro>

<SectionSubheading id="toc-what">{'\u{1F4E1}'} What is multipart/form-data?</SectionSubheading>

When a browser needs to send a file to a server, it can't just stuff raw bytes into a JSON string.
Instead, it uses an encoding called **multipart/form-data** ‚Äî a format defined in RFC 2046 <FnRef n={1} /> that bundles multiple pieces of data into a single HTTP request body, each separated by a unique boundary string.

Think of it like a letter with multiple attachments. Each "part" has its own headers
(name, filename, content type) and its own body (the actual data). The boundary string acts like
the divider between attachments ‚Äî the server reads until it hits the next boundary, processes that part, and moves on.

<Explainer title="Why not just base64-encode the file in JSON?">

Base64 encoding inflates the file size by ~33%. A 10 MB image becomes ~13.3 MB of text.
Multipart sends binary data as-is, keeping payloads lean. It also lets you mix text fields
and binary files in a single request without inventing your own encoding scheme.

</Explainer>

<SectionSubheading id="toc-anatomy">{'\u{1F4CB}'} Anatomy of a Request</SectionSubheading>

Here's what a multipart request actually looks like on the wire:

```http
POST /api/upload HTTP/1.1
Content-Type: multipart/form-data; boundary=----abc123

------abc123
Content-Disposition: form-data; name="userId"

42
------abc123
Content-Disposition: form-data; name="avatar"; filename="photo.jpg"
Content-Type: image/jpeg

<binary JPEG data>
------abc123--
```

Each part is delineated by the boundary string (`----abc123`).
Text fields contain their value directly. File fields include the filename and MIME type,
followed by the raw binary content. The final boundary ends with `--` to signal the end.

<SectionSubheading id="toc-pipeline">{'\u{1F680}'} The Upload Pipeline</SectionSubheading>

A file upload is a multi-step pipeline. Understanding each stage helps you debug failures and build a better UX.

<PipelineVisualization />

<SectionSubheading id="toc-implementation">{'\u2699\uFE0F'} Implementation</SectionSubheading>

A minimal upload function using the Fetch API <FnRef n={3} /> and FormData <FnRef n={2} />:

```typescript
async function uploadFile(file: File, metadata: Record<string, string>) {
  // 1. Client-side validation
  const MAX_SIZE = 10 * 1024 * 1024; // 10 MB
  if (file.size > MAX_SIZE) {
    throw new Error(`File too large: ${file.size} bytes (max ${MAX_SIZE})`);
  }

  // 2. Build FormData
  const formData = new FormData();
  formData.append("file", file);
  for (const [key, value] of Object.entries(metadata)) {
    formData.append(key, value);
  }

  // 3. Send ‚Äî do NOT set Content-Type manually
  const response = await fetch("/api/upload", {
    method: "POST",
    body: formData,
    // headers: { "Content-Type": "..." }  ‚Üê NEVER do this
  });

  if (!response.ok) {
    throw new Error(`Upload failed: ${response.status}`);
  }

  return response.json();
}
```

<Gotcha>

**The #1 multipart bug:** manually setting `Content-Type: multipart/form-data` will **break your upload**.
The browser needs to generate the boundary string and include it in the header.
When you set the header yourself, the boundary is missing and the server can't parse the body.

</Gotcha>

With TanStack Query, wrap this in a mutation:

```typescript
export function useUpload() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: async ({ file, meta }: { file: File; meta: Record<string, string> }) => {
      const formData = new FormData();
      formData.append("file", file);
      Object.entries(meta).forEach(([k, v]) => formData.append(k, v));

      const res = await fetch("/api/upload", {
        method: "POST",
        body: formData,
      });
      if (!res.ok) throw new Error("Upload failed");
      return res.json();
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ["files"] });
    },
  });
}
```

<SectionSubheading id="toc-demo">{'\u26A1'} Live Demo</SectionSubheading>

<MultipartDemo />

<SectionSubheading id="toc-when">{'\u25CE'} When to Use Multipart</SectionSubheading>

<WhenMultipart />

Data types that typically require multipart:

<DefinitionTable>
  <DefRow term={'\u{1F5BC}\uFE0F Images'}>Avatars, thumbnails, photo galleries, screenshots</DefRow>
  <DefRow term={'\u{1F4C4} Documents'}>PDFs, Word docs, spreadsheets, CSVs</DefRow>
  <DefRow term={'\u{1F3AC} Audio / Video'}>Recordings, music files, video clips</DefRow>
  <DefRow term={'\u{1F4E6} Archives'}>ZIP, tar.gz, or any compressed bundle</DefRow>
  <DefRow term={'\u{1F4DD} Rich form submissions'}>A profile update that includes both text fields AND a new avatar image</DefRow>
  <DefRow term={'\u{1F4CA} Bulk imports'}>CSV or Excel file with thousands of rows to process server-side</DefRow>
</DefinitionTable>

<Explainer title="Presigned URLs for large files">

For files over ~50 MB, consider using presigned URLs (S3, GCS, R2) so the client uploads directly to storage,
bypassing your server. Your server issues the URL; the client does a simple PUT. This avoids tying up your server
and lets you leverage CDN infrastructure for the heavy lifting.

</Explainer>

<SectionSubheading id="toc-packages">{'\u{1F50D}'} Vetting Upload Packages</SectionSubheading>

<PackageScorecard />

**Packages worth knowing about:**

<SectionNote>
**tus-js-client** ‚Äî implements the tus <FnRef n={6} /> open protocol for resumable uploads.
If the connection drops mid-upload, the client can resume from where it left off instead of starting over.
Excellent for large files and unreliable networks. Tradeoff: requires a tus-compatible server.
</SectionNote>

<SectionNote>
**Uppy** <FnRef n={7} /> ‚Äî an open-source upload suite with a modular plugin architecture. Includes drag-and-drop UI,
webcam capture, cloud imports (Google Drive, Dropbox), and multiple upload strategies (tus, XHR, S3 multipart).
Tradeoff: large bundle if you use the full UI ‚Äî cherry-pick only the plugins you need.
</SectionNote>

<SectionNote>
**ky / axios** ‚Äî general-purpose HTTP clients that handle FormData natively. axios has
built-in upload progress via `onUploadProgress` (uses XHR internally). ky is a tiny fetch wrapper ‚Äî
lighter, but no native upload progress. If you just need to POST a FormData with progress, axios is the pragmatic choice.
</SectionNote>

<Gotcha>

**Red flags when evaluating:** watch out for packages that haven't been updated in 2+ years,
have unresolved security advisories, vendor-lock you into a specific cloud provider, or require
you to set headers manually that the browser should handle. Always check the package's GitHub
issues for "CORS" and "boundary" ‚Äî those keywords surface the most common integration headaches.

</Gotcha>

<SectionSubheading id="toc-gotchas">{'\u26A0\uFE0F'} Common Gotchas</SectionSubheading>

**Setting Content-Type manually** ‚Äî This is the single most common multipart bug. When you pass a `FormData` body
to `fetch()`, the browser automatically generates the `Content-Type` header with a random boundary string.
If you override it, the boundary is lost and the server receives an unparseable body.

```typescript
// ‚ùå Don't
fetch("/api/upload", {
  method: "POST",
  headers: { "Content-Type": "multipart/form-data" }, // BREAKS IT
  body: formData,
});

// ‚úÖ Do
fetch("/api/upload", {
  method: "POST",
  // Let the browser handle Content-Type
  body: formData,
});
```

**CORS and preflight requests** ‚Äî Multipart uploads to a different origin trigger a CORS preflight (OPTIONS request).
Your server must respond to OPTIONS with the correct `Access-Control-Allow-*` headers.
Common mistake: allowing `Content-Type` in `Access-Control-Allow-Headers` but
forgetting `Access-Control-Allow-Methods: POST`.
Also: if you use a reverse proxy (nginx, Cloudflare), make sure it forwards the full request body without truncating it.
nginx's default `client_max_body_size` is 1 MB ‚Äî uploads larger than that will silently fail with a 413 error.

**File size limits across the stack** ‚Äî there are multiple layers that can reject a large file, and the error messages are often unhelpful:

<DefinitionTable>
  <DefRow term="Reverse proxy">nginx: client_max_body_size, Cloudflare: 100 MB free tier</DefRow>
  <DefRow term="Application server">Express: express.json() has a 100 KB default; multer <FnRef n={4} /> has its own limits</DefRow>
  <DefRow term="Cloud provider">S3 single PUT: 5 GB max; multipart upload: 5 TB max</DefRow>
  <DefRow term="Browser">No hard limit, but FormData with huge files will consume memory</DefRow>
</DefinitionTable>

**No upload progress with fetch()** ‚Äî the Fetch API does **not** support upload progress events in most browsers.
If you need a progress bar, you have three options: use `XMLHttpRequest` with
its `upload.onprogress` event, use a library like axios that wraps XHR
for you, or implement chunked uploads where you track progress yourself by counting completed chunks.

**Duplicate field names** ‚Äî `FormData.append()` allows duplicate keys.
Calling `formData.append("file", file1)` and then `formData.append("file", file2)` sends two parts
with the same name. Some server parsers only read the first value; others return an array.
Use `formData.set()` if you want to ensure only one value per key,
or use `formData.append("files", file)` with a plural key and configure
your server parser to expect an array (e.g., `multer.array("files")`).

**Memory pressure with large files** ‚Äî on the server, naive parsing (loading the entire file into memory) will crash your process on large uploads.
Use streaming parsers like busboy <FnRef n={5} /> (Node) or python-multipart (Python) that pipe file chunks directly to disk or cloud storage.
On the client, if you're reading files with `FileReader` for previews,
use `URL.createObjectURL(file)` instead ‚Äî it creates a reference to the file without copying it into memory.
Remember to call `URL.revokeObjectURL()` when you're done.
