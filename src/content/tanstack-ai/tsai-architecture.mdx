---
id: "tsai-architecture"
title: "Architecture ğŸ—ï¸"
guide: "tanstack-ai"
group: "Core Concepts"
---

<SectionTitle>{frontmatter.title}</SectionTitle>

<Toc>
  <TocLink id="toc-data-flow">Data flow</TocLink>
  <TocLink id="toc-overview">Architecture overview</TocLink>
</Toc>

<SectionIntro>
TanStack AI uses a clean client-server architecture connected via Server-Sent Events. The client manages UI state while the server handles LLM orchestration â€” they communicate via an open, documented protocol.
</SectionIntro>

<SectionSubheading id="toc-data-flow">Data flow</SectionSubheading>

Here's how data flows through a TanStack AI application:

<DefinitionTable>
  <DefRow term="React UI">Your components render messages and handle user input.</DefRow>
  <DefRow term="useChat()">The hook manages all client-side state â€” messages, loading, streaming, tool execution, and approval flows.</DefRow>
  <DefRow term="SSE Connection">Server-Sent Events stream data from server to client in real time. The protocol is open and documented.</DefRow>
  <DefRow term="chat() Server">The server-side function handles LLM orchestration and tool loops. Framework-agnostic â€” works with TanStack Start, Express, Hono, or plain Node.</DefRow>
  <DefRow term="LLM Adapter">The provider adapter translates between TanStack AI's unified interface and the specific LLM API (OpenAI, Anthropic, Gemini, Ollama).</DefRow>
</DefinitionTable>

<Explainer>
The `useChat` hook manages all client-side state â€” messages, loading, streaming, tool execution, and approval flows. The server's `chat()` function handles the LLM orchestration and tool loops. They communicate via an open, documented protocol.
</Explainer>

<SectionSubheading id="toc-overview">Architecture overview</SectionSubheading>

<CodeAccordion title="Architecture diagram" startOpen>
{`// CLIENT (React)                          SERVER (Node/Bun/etc)
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚  useChat() hook      â”‚  â†â”€â”€ SSE â”€â”€â†’   â”‚  chat() function       â”‚
// â”‚  â”œâ”€ messages         â”‚                 â”‚  â”œâ”€ adapter (provider) â”‚
// â”‚  â”œâ”€ sendMessage()    â”‚                 â”‚  â”œâ”€ tools[]            â”‚
// â”‚  â”œâ”€ isLoading        â”‚                 â”‚  â””â”€ streaming          â”‚
// â”‚  â”œâ”€ client tools     â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
// â”‚  â””â”€ tool approvals   â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`}
</CodeAccordion>

The architecture follows a clear separation of concerns:

<SectionList>
  <ColItem>**Client** â€” React components + the `useChat` hook handle rendering and user interaction. Client tools can execute directly in the browser.</ColItem>
  <ColItem>**Transport** â€” SSE (default), HTTP streams, or custom adapters like WebSocket carry streaming data between client and server.</ColItem>
  <ColItem>**Server** â€” The `chat()` function orchestrates the conversation loop, including multi-step tool execution. It's framework-agnostic.</ColItem>
  <ColItem>**Adapter** â€” Provider-specific adapters translate between TanStack AI's unified interface and each LLM's native API.</ColItem>
</SectionList>
