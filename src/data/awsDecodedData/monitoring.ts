import type { AwsService } from './types'

export const MONITORING_SERVICES: AwsService[] = [
  {
    id: 'cloudwatch',
    name: 'CloudWatch',
    fullName: 'Amazon CloudWatch',
    cat: 'monitoring',
    level: 'beginner',
    icon: '\u{1F4CA}',
    short: 'Monitoring and logging for everything in AWS. Collects metrics, stores logs, sets alarms, and creates dashboards.',
    analogy: 'The dashboard in your car \u2014 speed, fuel, engine temperature. Except it covers your entire cloud infrastructure.',
    detail: 'CloudWatch collects metrics (CPU usage, request counts, error rates), stores logs (application output, access logs), and lets you set alarms (email me if CPU > 80%). You can build dashboards to visualize everything in one place. Almost every AWS service automatically sends metrics to CloudWatch.',
    useCases: [
      'Monitoring EC2 CPU/memory usage',
      'Viewing Lambda function logs',
      'Setting alarms for error rates',
      'Creating operational dashboards',
    ],
    keyTerms: {
      Metric: 'A time-series data point (CPU%, request count, latency)',
      Alarm: 'A trigger based on a metric threshold',
      'Log Group': 'A collection of log streams from a service',
      Dashboard: 'A visual display of metrics and alarms',
    },
    pricing: 'Free tier: 10 custom metrics, 3 dashboards, basic monitoring. Very generous for small apps.',
    howItWorks:
      'CloudWatch works by receiving data from two primary channels: automatic metrics from AWS services and custom metrics you publish yourself. Every AWS service \u2014 from Lambda to RDS to API Gateway \u2014 pushes metrics to CloudWatch at regular intervals (typically every 1 or 5 minutes). You can also push custom metrics from your application code using the AWS SDK, which is how you track business-level KPIs like "orders per minute" or "signup conversion rate."\n\nLogs flow into CloudWatch Logs through log groups and log streams. When a Lambda function runs, its console.log output automatically goes to a log group named /aws/lambda/<function-name>. For EC2 or containers, you install the CloudWatch Agent to ship logs. Log Insights lets you query logs with a SQL-like syntax \u2014 this is where you\'ll spend most of your debugging time, searching for error patterns across thousands of log entries.\n\nAlarms tie everything together. You define a metric, a threshold, and an action. When CPU exceeds 80% for 5 consecutive minutes, the alarm transitions to ALARM state and can trigger an SNS notification (email/Slack), an Auto Scaling action, or a Lambda function. Composite alarms combine multiple conditions, so you can build sophisticated alerting like "high error rate AND high latency" before paging someone at 3 AM.',
    gotchas: [
      'CloudWatch metrics have a minimum resolution of 1 minute (or 1 second with detailed monitoring at extra cost). If your app needs sub-second metrics, you\'ll need a third-party tool like Datadog or Prometheus.',
      'Log retention defaults to "never expire," which means storage costs grow forever. Always set a retention policy on log groups \u2014 30 or 90 days is typical for non-compliance workloads.',
      'Custom metrics are charged per metric, not per data point. A metric with 5 dimensions counts as 5 metrics for billing. Cardinality explosions (e.g., one metric per user ID) can get expensive fast.',
      'CloudWatch Dashboards don\'t auto-refresh in the free tier \u2014 you have to manually refresh or pay for auto-refresh. For live monitoring, most teams use Grafana with CloudWatch as a data source instead.',
    ],
    whenNotToUse: [
      'If you need APM-style code-level profiling with flame graphs and method-level traces, CloudWatch is too coarse \u2014 use X-Ray for tracing or a dedicated APM like Datadog/New Relic.',
      'If you\'re building a multi-cloud setup, CloudWatch only covers AWS. Tools like Grafana, Datadog, or Prometheus give you a single pane across providers.',
      'If you need real-time streaming analytics on log data (sub-second), CloudWatch Logs has latency. Use Kinesis Data Streams or a self-managed ELK stack instead.',
    ],
    relatedServices: ['cloudtrail', 'x-ray', 'sns', 'lambda', 'ec2', 'config'],
    relatedGuides: [],
    code: '// Publishing a custom metric from Node.js\nimport { CloudWatch } from \'@aws-sdk/client-cloudwatch\';\n\nconst cw = new CloudWatch({});\n\nawait cw.putMetricData({\n  Namespace: \'MyApp\',\n  MetricData: [{\n    MetricName: \'OrdersPlaced\',\n    Value: 1,\n    Unit: \'Count\',\n    Dimensions: [{ Name: \'Environment\', Value: \'production\' }],\n  }],\n});',
    cliExample: '# Create an alarm that triggers when Lambda errors > 5 in 5 minutes\naws cloudwatch put-metric-alarm \\\n  --alarm-name "HighLambdaErrors" \\\n  --metric-name Errors \\\n  --namespace AWS/Lambda \\\n  --statistic Sum \\\n  --period 300 \\\n  --threshold 5 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 1 \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789:AlertsTopic',
    cdkExample: 'import * as cdk from \'aws-cdk-lib\';\nimport * as cloudwatch from \'aws-cdk-lib/aws-cloudwatch\';\nimport * as sns from \'aws-cdk-lib/aws-sns\';\nimport * as actions from \'aws-cdk-lib/aws-cloudwatch-actions\';\n\n// Create a dashboard\nconst dashboard = new cloudwatch.Dashboard(this, \'MyDashboard\', {\n  dashboardName: \'ProductionOverview\',\n});\n\n// Create an alarm on Lambda errors\nconst errorAlarm = new cloudwatch.Alarm(this, \'LambdaErrorAlarm\', {\n  metric: myFunction.metricErrors({ period: cdk.Duration.minutes(5) }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: \'Lambda error count exceeded threshold\',\n});\n\n// Send alarm to SNS topic\nerrorAlarm.addAlarmAction(new actions.SnsAction(alertTopic));',
  },
  {
    id: 'cloudtrail',
    name: 'CloudTrail',
    fullName: 'AWS CloudTrail',
    cat: 'monitoring',
    level: 'intermediate',
    icon: '\u{1F50D}',
    short: 'An audit log of every action taken in your AWS account. Who did what, when, and from where.',
    analogy: 'Security camera footage for your AWS account \u2014 records every action so you can review who did what.',
    detail: 'CloudTrail records API calls made in your AWS account. Every time someone (or some service) creates an EC2 instance, modifies an S3 bucket, or changes an IAM policy, CloudTrail logs it. Essential for security auditing, compliance, and troubleshooting "who broke production."',
    useCases: [
      'Security auditing and compliance',
      'Investigating incidents',
      'Tracking who made infrastructure changes',
    ],
    keyTerms: {
      Event: 'A single recorded API call',
      Trail: 'A configuration that sends events to S3 and/or CloudWatch',
      'Management Event': 'Control plane operations (create, delete, modify resources)',
    },
    pricing: 'Free: 90 days of management events. S3 storage charges for long-term retention.',
    howItWorks:
      'CloudTrail captures every API call made in your AWS account and records it as an event. This includes calls from the AWS Console (a human clicking buttons), the CLI, SDKs, and other AWS services acting on your behalf. Each event includes the identity of the caller (IAM user/role/service), the timestamp, the source IP, the service and action called, and the request/response parameters. Events are delivered to the CloudTrail console within about 15 minutes.\n\nA "trail" is a configuration that tells CloudTrail where to store events long-term. Without a trail, you only get 90 days of management events in the console. With a trail, events are delivered as JSON files to an S3 bucket (and optionally to CloudWatch Logs for real-time alerting). You can create organization-wide trails that capture events across all accounts in AWS Organizations \u2014 this is the standard setup for companies.\n\nCloudTrail distinguishes between management events (control plane operations like CreateBucket, RunInstances) and data events (data plane operations like S3 GetObject, Lambda Invoke). Management events are logged by default; data events must be explicitly enabled and can generate massive volumes. For frontend engineers, the most common use case is searching CloudTrail when a deployment breaks something \u2014 "who changed the CloudFront distribution at 3 PM?" \u2014 or when investigating unauthorized access.',
    gotchas: [
      'CloudTrail events are delivered with a delay of up to 15 minutes. It is not a real-time alerting system. For real-time detection, pipe events to CloudWatch Logs and set up metric filters.',
      'Data events (S3 object-level operations, Lambda invocations) are NOT logged by default and cost extra. If you need to audit who accessed a specific S3 object, you must explicitly enable data events for that bucket.',
      'CloudTrail log files in S3 are JSON, but they\'re compressed and nested. Querying them directly is painful \u2014 use Athena to run SQL queries on CloudTrail logs instead of trying to grep through raw files.',
      'A single trail applies to all regions by default (recommended), but if you create region-specific trails you might miss events in new regions AWS launches later.',
    ],
    whenNotToUse: [
      'Don\'t use CloudTrail for application-level logging (user actions in your app). It only tracks AWS API calls. Use CloudWatch Logs or a dedicated logging service for application events.',
      'Don\'t rely on CloudTrail for real-time threat detection alone. Use GuardDuty, which analyzes CloudTrail events along with VPC Flow Logs and DNS logs for automated threat detection.',
      'If you need to track resource configuration changes over time (not just who made the call), AWS Config is the better tool \u2014 it records the before/after state, not just the API call.',
    ],
    relatedServices: ['cloudwatch', 'config', 's3', 'iam', 'organizations'],
    relatedGuides: [],
    code: '// Querying CloudTrail events from Node.js\nimport { CloudTrail } from \'@aws-sdk/client-cloudtrail\';\n\nconst trail = new CloudTrail({});\n\nconst { Events } = await trail.lookupEvents({\n  LookupAttributes: [{\n    AttributeKey: \'EventName\',\n    AttributeValue: \'StopInstances\',\n  }],\n  StartTime: new Date(Date.now() - 24 * 60 * 60 * 1000),\n  EndTime: new Date(),\n});\n\nEvents?.forEach(e => {\n  console.log(`${e.EventTime} - ${e.Username} - ${e.EventName}`);\n});',
    cliExample: '# Look up who stopped EC2 instances in the last 24 hours\naws cloudtrail lookup-events \\\n  --lookup-attributes AttributeKey=EventName,AttributeValue=StopInstances \\\n  --start-time "2024-01-15T00:00:00Z" \\\n  --query \'Events[].{Time:EventTime,User:Username,Event:EventName}\' \\\n  --output table',
    cdkExample: 'import * as cloudtrail from \'aws-cdk-lib/aws-cloudtrail\';\nimport * as s3 from \'aws-cdk-lib/aws-s3\';\nimport * as logs from \'aws-cdk-lib/aws-logs\';\n\n// Create an S3 bucket for trail storage\nconst trailBucket = new s3.Bucket(this, \'TrailBucket\', {\n  encryption: s3.BucketEncryption.S3_MANAGED,\n  lifecycleRules: [{ expiration: cdk.Duration.days(365) }],\n});\n\n// Create a CloudTrail trail\nconst trail = new cloudtrail.Trail(this, \'AuditTrail\', {\n  bucket: trailBucket,\n  sendToCloudWatchLogs: true,\n  cloudWatchLogGroup: new logs.LogGroup(this, \'TrailLogs\', {\n    retention: logs.RetentionDays.ONE_YEAR,\n  }),\n  isMultiRegionTrail: true,\n  includeGlobalServiceEvents: true,\n});',
  },
  {
    id: 'systems-manager',
    name: 'Systems Manager',
    fullName: 'AWS Systems Manager',
    cat: 'monitoring',
    level: 'intermediate',
    icon: '\u{1F39B}\uFE0F',
    short: 'A unified tool to manage your infrastructure at scale. Run commands, patch servers, store configuration, manage parameters.',
    analogy: 'A universal remote control for all your AWS infrastructure \u2014 one place to manage everything.',
    detail: 'Systems Manager bundles many operational tools: Parameter Store (secure config/secrets), Session Manager (SSH without opening ports), Run Command (execute scripts across instances), Patch Manager (automated OS patching), and more. It\'s the Swiss Army knife of AWS operations.',
    useCases: [
      'Storing application configuration (Parameter Store)',
      'Remote access to EC2 without SSH keys',
      'Automated patching across fleets of servers',
    ],
    keyTerms: {
      'Parameter Store': 'Key-value store for configuration and secrets (free alternative to Secrets Manager)',
      'Session Manager': 'Browser-based shell access to EC2',
      'Run Command': 'Execute scripts on multiple instances at once',
    },
    pricing: 'Most features are free. Advanced Parameter Store: $0.05/parameter/month.',
    howItWorks:
      'Systems Manager is actually a collection of 15+ tools under one umbrella. The most relevant for frontend engineers are Parameter Store, Session Manager, and Run Command. Parameter Store is a hierarchical key-value store where you keep configuration like API keys, database URLs, and feature flags. It supports plain text and encrypted values (via KMS), versioning, and path-based organization (e.g., /myapp/prod/db-url). Your Lambda functions and containers read these at startup instead of hardcoding secrets.\n\nSession Manager provides browser-based or CLI-based shell access to EC2 instances without opening SSH ports (port 22) in your security groups. This is a huge security win \u2014 no SSH keys to manage, no inbound ports to open, and every session is logged to CloudTrail and optionally S3. For frontend engineers who occasionally need to debug something on an EC2 instance, Session Manager is the recommended approach.\n\nRun Command lets you execute scripts (shell commands, PowerShell, Python) across dozens or hundreds of instances simultaneously without SSH-ing into each one. You define a "document" (a script template), target instances by tag or resource group, and execute. Results stream back to the console. Patch Manager builds on this to automate OS-level patching on a schedule \u2014 you define a maintenance window and a patch baseline, and Systems Manager handles the rest.',
    gotchas: [
      'Parameter Store has two tiers: Standard (free, 10,000 params max, 4 KB per value) and Advanced ($0.05/param/month, 100,000 params, 8 KB per value). Most apps fit comfortably in Standard, but if you store large JSON configs, you\'ll hit the 4 KB limit.',
      'Session Manager requires the SSM Agent to be installed on your EC2 instances. Amazon Linux 2 and Ubuntu 20.04+ come with it pre-installed, but older AMIs or custom images may not have it \u2014 no agent means no Session Manager.',
      'Parameter Store values fetched by Lambda are not automatically refreshed during warm starts. If you change a parameter, already-running Lambda instances will use the old value until they cold-start again. Use the AWS Parameters and Secrets Lambda Extension to add a caching layer with configurable TTL.',
      'The free tier of Parameter Store doesn\'t support parameter policies (auto-expiration, change notifications). If you need a parameter to auto-rotate or alert you when it changes, you need the Advanced tier.',
    ],
    whenNotToUse: [
      'If you need automatic secret rotation (e.g., database passwords), use Secrets Manager instead. Parameter Store can store secrets but doesn\'t rotate them automatically.',
      'If you need a full configuration management system with drift detection and remediation (like Ansible or Chef), Systems Manager\'s State Manager is limited. Consider proper IaC tools like CDK or Terraform.',
      'If you need to store secrets larger than 8 KB (like TLS certificates or large JSON configs), use Secrets Manager (up to 64 KB) or store them in S3 with encryption.',
    ],
    relatedServices: ['secrets-manager', 'ec2', 'lambda', 'kms', 'cloudwatch'],
    relatedGuides: [],
    code: '// Reading a parameter from Parameter Store in Node.js\nimport { SSM } from \'@aws-sdk/client-ssm\';\n\nconst ssm = new SSM({});\n\n// Get a single parameter\nconst { Parameter } = await ssm.getParameter({\n  Name: \'/myapp/prod/api-key\',\n  WithDecryption: true,  // decrypts SecureString params\n});\nconsole.log(Parameter?.Value);\n\n// Get multiple parameters by path\nconst { Parameters } = await ssm.getParametersByPath({\n  Path: \'/myapp/prod/\',\n  WithDecryption: true,\n  Recursive: true,\n});\nParameters?.forEach(p => console.log(p.Name, p.Value));',
    cliExample: '# Store an encrypted parameter\naws ssm put-parameter \\\n  --name "/myapp/prod/db-password" \\\n  --value "super-secret-password" \\\n  --type SecureString \\\n  --overwrite\n\n# Read it back\naws ssm get-parameter \\\n  --name "/myapp/prod/db-password" \\\n  --with-decryption \\\n  --query "Parameter.Value" \\\n  --output text',
    cdkExample: 'import * as ssm from \'aws-cdk-lib/aws-ssm\';\n\n// Create a plain text parameter\nconst apiUrl = new ssm.StringParameter(this, \'ApiUrl\', {\n  parameterName: \'/myapp/prod/api-url\',\n  stringValue: \'https://api.example.com/v2\',\n  description: \'Production API endpoint\',\n  tier: ssm.ParameterTier.STANDARD,\n});\n\n// Read an existing parameter in another stack\nconst dbPassword = ssm.StringParameter.valueForSecureStringParameter(\n  this, \'/myapp/prod/db-password\', 1  // version\n);\n\n// Grant read access to a Lambda function\napiUrl.grantRead(myFunction);',
  },
  {
    id: 'x-ray',
    name: 'X-Ray',
    fullName: 'AWS X-Ray',
    cat: 'monitoring',
    level: 'intermediate',
    icon: '\u{1FA7B}',
    short: 'Distributed tracing. See exactly how a request flows through your microservices and find performance bottlenecks.',
    analogy: 'A GPS tracker for your requests \u2014 follow each one as it hops between services and see where it gets stuck.',
    detail: 'X-Ray traces requests as they travel through your application. It creates a "service map" showing how services connect and where latency occurs. You can drill into individual traces to see exactly which service call was slow, which Lambda function errored, or which database query took too long.',
    useCases: [
      'Finding performance bottlenecks in microservices',
      'Debugging errors across distributed services',
      'Understanding service dependencies',
    ],
    keyTerms: {
      Trace: 'The full journey of a single request',
      Segment: 'One service\'s portion of a trace',
      'Service Map': 'Visual diagram of how services connect',
    },
    pricing: 'Free tier: 100K traces recorded + 1M traces retrieved/month. After: $5/million traces.',
    howItWorks:
      'X-Ray works by propagating a unique trace ID through every service a request touches. When a request enters your system (e.g., hitting API Gateway), X-Ray assigns it a trace ID. As the request flows to Lambda, then to DynamoDB, then to another Lambda, each service adds a "segment" to that trace. Segments contain timing data, HTTP status codes, error messages, and custom annotations you add. Subsegments break down the work within a service \u2014 for example, a Lambda function\'s segment might have subsegments for "DynamoDB query" and "S3 upload."\n\nInstrumentation is how you enable X-Ray in your code. For AWS SDK calls, the X-Ray SDK wraps the AWS SDK and automatically creates subsegments for every AWS API call. For HTTP calls to external services, you wrap your HTTP client (axios, fetch). For Lambda, you just enable "active tracing" in the function configuration \u2014 no code changes needed for basic tracing. API Gateway, AppSync, and other services have built-in X-Ray integration you can toggle on.\n\nThe X-Ray console provides two key views: the Service Map and the Trace List. The Service Map is a visual graph of all your services and how they connect, with color-coded health indicators (green/yellow/red) and average latencies on each edge. The Trace List lets you filter traces by URL, status code, response time, or custom annotations, then drill into individual traces to see the full waterfall of calls. This is where you\'ll find that one downstream service adding 800ms to your P99 latency.',
    gotchas: [
      'X-Ray uses sampling to control costs and volume \u2014 by default, it records the first request each second and 5% of additional requests. This means you won\'t see every single request, which can make debugging intermittent issues tricky. You can customize sampling rules per route or service.',
      'The X-Ray SDK and the newer AWS Distro for OpenTelemetry (ADOT) are two different instrumentation approaches. AWS is moving toward ADOT/OpenTelemetry as the standard. For new projects, prefer ADOT since it\'s vendor-neutral and works with other tracing backends.',
      'X-Ray traces have a maximum duration of 24 hours. Long-running batch jobs or Step Functions executions that exceed this won\'t have complete traces.',
      'Enabling X-Ray on Lambda adds a small amount of cold start latency (typically 50-100ms for the first invocation) because the X-Ray daemon needs to initialize. For latency-critical functions, measure the impact.',
    ],
    whenNotToUse: [
      'If you\'re running a monolithic application with no microservices, X-Ray provides little value. Standard application profiling tools (Chrome DevTools, Node.js --inspect, or a profiler) are more appropriate.',
      'If you need long-term trace storage and advanced analytics, X-Ray retains traces for only 30 days. Use a dedicated observability platform like Jaeger, Honeycomb, or Datadog APM for historical analysis.',
      'If your architecture is entirely client-side (a static SPA calling third-party APIs), X-Ray won\'t help. It instruments server-side AWS services. Use browser-based tools like Sentry or LogRocket for frontend performance.',
    ],
    relatedServices: ['cloudwatch', 'lambda', 'api-gateway', 'step-functions', 'ecs'],
    relatedGuides: [],
    code: '// Instrumenting an Express app with X-Ray\nimport AWSXRay from \'aws-xray-sdk-core\';\nimport express from \'express\';\n\nconst app = express();\n\n// Capture all AWS SDK calls automatically\nAWSXRay.captureAWS(require(\'aws-sdk\'));\n\n// Open X-Ray segment for each request\napp.use(AWSXRay.express.openSegment(\'MyApp\'));\n\napp.get(\'/api/users\', async (req, res) => {\n  // Any AWS SDK call here is automatically traced\n  const users = await dynamodb.scan({ TableName: \'Users\' });\n  res.json(users.Items);\n});\n\n// Close X-Ray segment\napp.use(AWSXRay.express.closeSegment());',
    cliExample: '# Get a summary of traces from the last 10 minutes\naws xray get-trace-summaries \\\n  --start-time $(date -u -d "10 minutes ago" +%s) \\\n  --end-time $(date -u +%s) \\\n  --filter-expression \'service("MyApp") AND responsetime > 2\' \\\n  --query \'TraceSummaries[].{Id:Id,Duration:Duration,Status:Http.HttpStatus}\' \\\n  --output table',
    cdkExample: 'import * as lambda from \'aws-cdk-lib/aws-lambda\';\nimport * as apigateway from \'aws-cdk-lib/aws-apigateway\';\n\n// Enable X-Ray tracing on a Lambda function\nconst fn = new lambda.Function(this, \'TracedFunction\', {\n  runtime: lambda.Runtime.NODEJS_20_X,\n  handler: \'index.handler\',\n  code: lambda.Code.fromAsset(\'lambda\'),\n  tracing: lambda.Tracing.ACTIVE,  // enables X-Ray\n});\n\n// Enable X-Ray on API Gateway\nconst api = new apigateway.RestApi(this, \'TracedApi\', {\n  deployOptions: {\n    tracingEnabled: true,  // enables X-Ray for the stage\n  },\n});\n\napi.root.addResource(\'users\').addMethod(\'GET\', \n  new apigateway.LambdaIntegration(fn)\n);',
  },
  {
    id: 'config',
    name: 'Config',
    fullName: 'AWS Config',
    cat: 'monitoring',
    level: 'advanced',
    icon: '\u2699\uFE0F',
    short: 'Tracks and audits all your AWS resource configurations over time. Answers: "What changed and when?"',
    analogy: 'A time machine for your infrastructure \u2014 see the exact state of any resource at any point in history.',
    detail: 'AWS Config continuously records configuration changes to your resources. It can evaluate rules (is this S3 bucket public? Is encryption enabled?) and flag non-compliant resources. It\'s essential for compliance, governance, and change management.',
    useCases: [
      'Compliance auditing',
      'Change tracking',
      'Enforcing security policies',
      'Resource inventory',
    ],
    keyTerms: {
      'Configuration Item': 'A snapshot of a resource\'s configuration at a point in time',
      'Config Rule': 'An evaluation that checks resources for compliance',
    },
    pricing: '$0.003/configuration item recorded. $0.001/rule evaluation.',
    howItWorks:
      'AWS Config works by continuously monitoring your AWS resources and recording their configuration as "configuration items" (CIs). Every time a resource changes \u2014 a security group rule is added, an S3 bucket policy is modified, an EC2 instance is resized \u2014 Config captures the new state and stores it. You get a complete timeline of every resource\'s configuration history, which means you can answer questions like "what did this security group look like last Tuesday at 2 PM?"\n\nConfig Rules are the compliance engine. AWS provides over 300 managed rules out of the box (e.g., "S3 buckets must not be public," "EBS volumes must be encrypted," "EC2 instances must be in a VPC"). You can also write custom rules as Lambda functions for organization-specific policies. Rules are evaluated automatically when a resource changes (triggered mode) or on a schedule (periodic mode). Non-compliant resources show up in the Config dashboard with details about which rule they violate.\n\nFor frontend engineers, Config is most relevant when your team enforces infrastructure policies. If you deploy a CloudFront distribution without HTTPS, or create an S3 bucket without encryption, Config flags it. Conformance packs let you deploy bundles of related rules \u2014 for example, the "AWS Foundational Security Best Practices" pack covers dozens of common security checks. Config also integrates with Systems Manager for automated remediation: when a non-compliant resource is detected, an SSM Automation runbook can fix it automatically.',
    gotchas: [
      'Config charges per configuration item recorded AND per rule evaluation. In a large account with hundreds of resources and dozens of rules, costs can add up quickly. Use resource type filters to limit recording to the resource types you actually care about.',
      'Config Rules evaluate compliance but do NOT prevent non-compliant resources from being created. For preventive controls, use Service Control Policies (SCPs) in Organizations or IAM permission boundaries. Config is detective, not preventive.',
      'The Config recorder must be turned on to start tracking. It is NOT enabled by default in new accounts. If you turn it on today, you have no history from before today. Enable it early in your AWS journey.',
      'Config\'s configuration timeline can differ from CloudTrail\'s event history. CloudTrail tells you who made a change; Config tells you what the resource looked like before and after. You often need both to get the full picture.',
    ],
    whenNotToUse: [
      'If you just need to know who made a change (the actor), CloudTrail is sufficient. Config is for tracking the resource state itself, which is a different (complementary) concern.',
      'If you want to prevent non-compliant resources from being created in the first place, use infrastructure-as-code (CDK/Terraform) with policy-as-code tools like OPA or CDK Nag. Config detects after the fact.',
      'If you have a small, simple AWS account with a handful of resources, Config\'s per-item pricing may not be worth it. Manual reviews or simple CloudTrail searches may suffice.',
    ],
    relatedServices: ['cloudtrail', 'cloudwatch', 'systems-manager', 'organizations', 'lambda'],
    relatedGuides: [],
    code: '// Querying Config for resource configuration history\nimport { ConfigService } from \'@aws-sdk/client-config-service\';\n\nconst config = new ConfigService({});\n\n// Get configuration history of a specific resource\nconst { configurationItems } = await config.getResourceConfigHistory({\n  resourceType: \'AWS::S3::Bucket\',\n  resourceId: \'my-app-bucket\',\n  limit: 10,\n});\n\nconfigurationItems?.forEach(item => {\n  console.log(`${item.configurationItemCaptureTime}:`, \n    JSON.parse(item.configuration ?? \'{}\'));\n});\n\n// Check compliance status of all rules\nconst { ComplianceByConfigRules } = await config.describeComplianceByConfigRule({});\nComplianceByConfigRules?.forEach(rule => {\n  console.log(`${rule.ConfigRuleName}: ${rule.Compliance?.ComplianceType}`);\n});',
    cliExample: '# List all non-compliant resources across all Config rules\naws configservice describe-compliance-by-config-rule \\\n  --compliance-types NON_COMPLIANT \\\n  --query \'ComplianceByConfigRules[].{Rule:ConfigRuleName,Status:Compliance.ComplianceType}\' \\\n  --output table\n\n# View configuration history for a specific S3 bucket\naws configservice get-resource-config-history \\\n  --resource-type AWS::S3::Bucket \\\n  --resource-id my-app-bucket \\\n  --limit 5',
    cdkExample: 'import * as config from \'aws-cdk-lib/aws-config\';\n\n// Managed rule: S3 buckets must have encryption enabled\nnew config.ManagedRule(this, \'S3EncryptionRule\', {\n  identifier: config.ManagedRuleIdentifiers.S3_BUCKET_SERVER_SIDE_ENCRYPTION_ENABLED,\n  configRuleName: \'s3-encryption-required\',\n});\n\n// Managed rule: S3 buckets must not be public\nnew config.ManagedRule(this, \'S3PublicRule\', {\n  identifier: config.ManagedRuleIdentifiers.S3_BUCKET_PUBLIC_READ_PROHIBITED,\n  configRuleName: \'s3-no-public-read\',\n});\n\n// Custom rule using a Lambda function\nnew config.CustomRule(this, \'CustomTagRule\', {\n  lambdaFunction: tagCheckerFunction,\n  configurationChanges: true,  // trigger on resource changes\n  ruleScope: config.RuleScope.fromResources([\n    config.ResourceType.EC2_INSTANCE,\n  ]),\n});',
  },
  {
    id: 'backup',
    name: 'Backup',
    fullName: 'AWS Backup',
    cat: 'monitoring',
    level: 'intermediate',
    icon: '\u{1F4BE}',
    short: 'Centralized backup management. Define backup policies once and apply them across EC2, RDS, EFS, DynamoDB, and more.',
    analogy: 'A fire alarm system for your data \u2014 scheduled, automated, and protects everything under one roof.',
    detail: 'AWS Backup lets you create backup plans that automatically back up resources on a schedule. Define retention rules, copy to another region for disaster recovery, and audit backup compliance. Supports EC2, EBS, RDS, DynamoDB, EFS, FSx, and more.',
    useCases: [
      'Automated daily backups',
      'Cross-region disaster recovery',
      'Compliance with data retention policies',
    ],
    keyTerms: {
      'Backup Plan': 'Defines what to back up, how often, and retention',
      'Backup Vault': 'Storage container for backups',
      'Recovery Point': 'A single backup snapshot',
    },
    pricing: 'Pay for storage consumed: $0.05/GB/month for warm storage.',
    howItWorks:
      'AWS Backup provides a centralized, policy-driven approach to backing up AWS resources. You create a backup plan that specifies: which resources to back up (via tags or resource ARNs), when to back up (cron schedule or frequency), how long to keep each backup (retention period), and where to store them (backup vault). A backup vault is an encrypted container that holds your recovery points \u2014 you can have multiple vaults with different access policies, including vaults that use Vault Lock to make backups immutable (even an admin can\'t delete them).\n\nWhen a backup job runs, AWS Backup coordinates with the underlying service to create a snapshot. For EBS, it triggers an EBS snapshot. For RDS, it creates an RDS snapshot. For DynamoDB, it uses on-demand backup. The key benefit is that you manage all of these through one interface instead of configuring backup schedules separately in each service. Cross-region and cross-account copy rules let you automatically replicate backups to a secondary region or a separate AWS account for disaster recovery.\n\nBackup auditing and compliance are built in. AWS Backup Audit Manager lets you define frameworks (e.g., "all production databases must be backed up daily with 30-day retention") and monitors whether your resources are compliant. This integrates with AWS Config, so you can see backup compliance alongside your other governance rules. For frontend engineers, the typical interaction with Backup is ensuring your RDS databases and DynamoDB tables are included in the team\'s backup plan, and occasionally restoring from a recovery point when something goes wrong during a migration or deployment.',
    gotchas: [
      'AWS Backup costs include both storage and data transfer. Cross-region copies incur data transfer charges in addition to storage costs in the destination region. For large databases, this can be significant.',
      'Not all resource types support all backup features. For example, DynamoDB backups made through AWS Backup don\'t support point-in-time recovery (PITR) \u2014 you need to enable PITR separately on the DynamoDB table itself.',
      'Restoring from a backup creates a NEW resource, not an in-place restore. If you restore an RDS database, you get a new RDS instance with a new endpoint. Your application config needs to be updated to point to the new instance.',
      'Vault Lock is irreversible once the cooling-off period ends. If you enable Vault Lock with a 365-day minimum retention and lock it, you CANNOT delete those backups for a full year, even if the data is no longer needed. Test with short retention periods first.',
    ],
    whenNotToUse: [
      'If you need continuous, point-in-time recovery (restore to any second), use the service\'s native PITR feature instead (RDS PITR, DynamoDB PITR). AWS Backup creates periodic snapshots, not continuous backups.',
      'If you\'re backing up application-level data (user uploads, generated files) that lives outside AWS-managed services, AWS Backup won\'t help. Use S3 versioning and lifecycle policies, or a custom backup script.',
      'If your disaster recovery plan requires sub-minute RPO (recovery point objective), AWS Backup\'s minimum frequency is every hour. Use database replication (RDS Multi-AZ, DynamoDB Global Tables) for near-zero RPO.',
    ],
    relatedServices: ['s3', 'rds', 'dynamodb', 'ebs', 'efs', 'config'],
    relatedGuides: [],
    code: '// Listing recovery points for a resource\nimport { Backup } from \'@aws-sdk/client-backup\';\n\nconst backup = new Backup({});\n\n// List all recovery points in a vault\nconst { RecoveryPoints } = await backup.listRecoveryPointsByBackupVault({\n  BackupVaultName: \'Default\',\n});\n\nRecoveryPoints?.forEach(rp => {\n  console.log(\n    `${rp.ResourceType} | ${rp.CreationDate} | ${rp.Status}`\n  );\n});\n\n// Start a restore job\nawait backup.startRestoreJob({\n  RecoveryPointArn: \'arn:aws:backup:us-east-1:123456789:recovery-point:abc-123\',\n  Metadata: {\n    // Metadata varies by resource type\n    \'DynamoDBTableName\': \'MyRestoredTable\',\n  },\n  IamRoleArn: \'arn:aws:iam::123456789:role/BackupRestoreRole\',\n});',
    cliExample: '# List all backup plans\naws backup list-backup-plans \\\n  --query \'BackupPlansList[].{Name:BackupPlanName,Id:BackupPlanId}\' \\\n  --output table\n\n# List recovery points in the default vault\naws backup list-recovery-points-by-backup-vault \\\n  --backup-vault-name Default \\\n  --query \'RecoveryPoints[].{Type:ResourceType,Created:CreationDate,Status:Status}\' \\\n  --output table',
    cdkExample: 'import * as backup from \'aws-cdk-lib/aws-backup\';\nimport * as events from \'aws-cdk-lib/aws-events\';\n\n// Create a backup vault\nconst vault = new backup.BackupVault(this, \'BackupVault\', {\n  backupVaultName: \'ProductionVault\',\n  removalPolicy: cdk.RemovalPolicy.RETAIN,\n});\n\n// Create a backup plan\nconst plan = new backup.BackupPlan(this, \'BackupPlan\', {\n  backupPlanName: \'DailyBackupPlan\',\n  backupVault: vault,\n  backupPlanRules: [\n    new backup.BackupPlanRule({\n      ruleName: \'DailyBackup\',\n      scheduleExpression: events.Schedule.cron({ hour: \'3\', minute: \'0\' }),\n      deleteAfter: cdk.Duration.days(30),\n      moveToColdStorageAfter: cdk.Duration.days(7),\n    }),\n  ],\n});\n\n// Back up all resources tagged with backup:true\nplan.addSelection(\'TaggedResources\', {\n  resources: [\n    backup.BackupResource.fromTag(\'backup\', \'true\'),\n  ],\n});',
  },
]
