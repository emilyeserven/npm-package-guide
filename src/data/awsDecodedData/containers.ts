import type { AwsService } from './types'

export const CONTAINER_SERVICES: AwsService[] = [
  {
    id: 'ecs',
    name: 'ECS',
    fullName: 'Elastic Container Service',
    cat: 'containers',
    level: 'intermediate',
    icon: '\u{1F433}',
    short: 'Run Docker containers at scale. You define your container images, and ECS handles placing them on servers and keeping them running.',
    analogy: 'A shipping port manager that loads containers onto ships, monitors them, and replaces any that fall overboard.',
    detail: 'ECS runs your Docker containers on a cluster of EC2 instances or on Fargate (serverless). You write a "task definition" (a JSON recipe for your container \u2014 image, CPU, memory, ports, env vars), and ECS ensures the right number of containers are always running. It integrates with ALB for load balancing and CloudWatch for logging.',
    useCases: [
      'Running microservices',
      'Batch processing jobs',
      'Any Dockerized application at scale',
    ],
    keyTerms: {
      Task: 'A running instance of your container(s)',
      Service: 'Ensures a specified number of tasks are always running',
      'Task Definition': 'A blueprint for your container (image, resources, ports)',
    },
    pricing: 'No additional charge for ECS itself \u2014 you pay for EC2 or Fargate resources.',
    code: '// Dockerfile for a typical frontend build\nFROM node:20-alpine AS build\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\nFROM nginx:alpine\nCOPY --from=build /app/dist /usr/share/nginx/html\nEXPOSE 80',
    howItWorks: 'ECS operates on a cluster model. A cluster is a logical grouping of compute resources where your containers run. You create a task definition \u2014 essentially a JSON document that tells ECS which Docker image to pull, how much CPU and memory to allocate, which ports to expose, what environment variables to inject, and where to send logs. Think of a task definition as your docker-compose.yml equivalent in the AWS world.\n\nWhen you create an ECS service, you tell it "keep N copies of this task definition running at all times." The ECS scheduler finds available capacity in your cluster (either EC2 instances or Fargate) and places tasks accordingly. If a container crashes, the service scheduler automatically launches a replacement. If you attach an Application Load Balancer, ECS automatically registers and deregisters container instances as they come and go.\n\nFor frontend engineers, the typical workflow is: build your app into a Docker image (e.g., a React app served by nginx), push it to ECR, then update your ECS service to deploy the new image. ECS performs rolling deployments by default \u2014 it spins up new containers with the updated image before draining the old ones, so there is zero downtime.',
    gotchas: [
      'Task definitions are immutable \u2014 every change creates a new revision. You update a service to point to the new revision, not edit the existing one.',
      'Container port mappings on EC2 launch type require careful management. If you use dynamic port mapping with an ALB, make sure the security group allows the ephemeral port range (32768-65535).',
      'ECS does not automatically scale the underlying EC2 instances \u2014 you need a separate Capacity Provider or Auto Scaling Group for that. Fargate launch type avoids this problem entirely.',
      'Log configuration must be set in the task definition. If you forget to configure the awslogs driver, your container logs vanish and debugging becomes painful.',
    ],
    whenNotToUse: [
      'Simple static sites or SPAs \u2014 use S3 + CloudFront instead. There is no need for container orchestration to serve static files.',
      'Short-lived, event-driven functions (< 15 min) \u2014 Lambda is cheaper and simpler than maintaining a full container service for small tasks.',
      'Your team already has deep Kubernetes expertise and multi-cloud requirements \u2014 EKS provides more portability and a broader ecosystem.',
    ],
    relatedServices: ['fargate', 'ecr', 'elb', 'cloudwatch', 'codepipeline'],
    relatedGuides: ['kubernetes'],
    cliExample: '# Register a task definition and create a service\naws ecs register-task-definition \\\n  --family my-frontend \\\n  --container-definitions \'[{\n    "name": "web",\n    "image": "123456789.dkr.ecr.us-east-1.amazonaws.com/my-app:latest",\n    "portMappings": [{"containerPort": 80}],\n    "memory": 512,\n    "cpu": 256\n  }]\'\n\naws ecs create-service \\\n  --cluster my-cluster \\\n  --service-name my-frontend-svc \\\n  --task-definition my-frontend \\\n  --desired-count 2 \\\n  --launch-type FARGATE',
    cdkExample: 'import * as ecs from \'aws-cdk-lib/aws-ecs\';\nimport * as ecsPatterns from \'aws-cdk-lib/aws-ecs-patterns\';\n\n// One-liner: ALB + Fargate + auto-scaling\nconst service = new ecsPatterns.ApplicationLoadBalancedFargateService(\n  this, \'FrontendService\', {\n    cluster,\n    taskImageOptions: {\n      image: ecs.ContainerImage.fromEcrRepository(repo, \'latest\'),\n      containerPort: 80,\n    },\n    desiredCount: 2,\n    publicLoadBalancer: true,\n  }\n);',
  },
  {
    id: 'fargate',
    name: 'Fargate',
    fullName: 'AWS Fargate',
    cat: 'containers',
    level: 'intermediate',
    icon: '\u{1F6A2}',
    short: 'Serverless containers. Run Docker containers without managing servers. Like Lambda, but for containers instead of functions.',
    analogy: 'A cruise ship that appears when you need it \u2014 you describe your container, and Fargate gives it a place to run. No ship maintenance.',
    detail: 'Fargate is a compute engine for ECS (and EKS) that removes the need to manage EC2 instances. You specify CPU and memory for your container, and Fargate provisions the right resources. It\'s "serverless containers" \u2014 you focus on your application, not infrastructure.',
    useCases: [
      'Microservices without managing servers',
      'Background workers and job processors',
      'Teams that know Docker but don\'t want to manage EC2',
    ],
    keyTerms: {
      'Launch Type': 'Whether ECS runs on EC2 or Fargate',
      'vCPU/Memory': 'The resource allocation for your container',
    },
    pricing: 'Pay per vCPU and memory per second. ~$0.04048/vCPU/hr, ~$0.004445/GB/hr.',
    code: '// Task definition snippet for Fargate\n{\n  "family": "my-frontend",\n  "networkMode": "awsvpc",\n  "requiresCompatibilities": ["FARGATE"],\n  "cpu": "256",\n  "memory": "512",\n  "containerDefinitions": [{\n    "name": "web",\n    "image": "my-app:latest",\n    "portMappings": [{ "containerPort": 3000 }]\n  }]\n}',
    howItWorks: 'Fargate is not a standalone service \u2014 it is a launch type (compute engine) for ECS and EKS. When you choose Fargate, AWS provisions a lightweight micro-VM for each task using Firecracker (the same technology that powers Lambda). Each task gets its own isolated kernel, so there is no multi-tenant container sharing. You never see or manage the underlying host.\n\nThe key constraint is that Fargate tasks must use "awsvpc" network mode, which means every task gets its own Elastic Network Interface (ENI) and private IP address within your VPC. This is great for security \u2014 you can attach security groups directly to individual tasks \u2014 but it means you need enough IP addresses in your subnets. For a frontend app serving traffic, you would typically place the Fargate tasks in private subnets behind an Application Load Balancer in public subnets.\n\nFargate supports Spot pricing (Fargate Spot) that can save up to 70% for fault-tolerant workloads like build jobs or batch processing. For your production frontend services, stick with regular Fargate. Scaling works by simply changing the desired count on your ECS service \u2014 Fargate spins up new micro-VMs in seconds without you managing any capacity.',
    gotchas: [
      'CPU and memory combos are fixed tiers (e.g., 0.25 vCPU only allows 0.5/1/2 GB memory). You cannot pick arbitrary values \u2014 check the Fargate task size table in the docs.',
      'Each Fargate task gets its own ENI, which counts against your VPC subnet IP limit. If you run many tasks, ensure your subnets have a large enough CIDR block.',
      'Ephemeral storage defaults to 20 GB (expandable to 200 GB). If your build process creates large intermediate files, you may need to increase this in the task definition.',
      'Fargate tasks in private subnets need a NAT Gateway (or VPC endpoints) to pull images from ECR or reach the internet. A missing NAT Gateway is the number one reason Fargate tasks get stuck in PROVISIONING.',
    ],
    whenNotToUse: [
      'GPU workloads or custom kernel modules \u2014 Fargate does not support GPU instances or host-level configuration. Use EC2 launch type instead.',
      'Consistently high-utilization, long-running workloads where you can commit to Reserved Instances \u2014 EC2 can be significantly cheaper at scale with reservations.',
      'Applications that need to run as privileged containers or mount host volumes \u2014 Fargate\'s isolation model does not allow these operations.',
    ],
    relatedServices: ['ecs', 'eks', 'ecr', 'vpc', 'elb'],
    relatedGuides: ['kubernetes'],
    cliExample: '# Run a one-off Fargate task (great for migrations, scripts)\naws ecs run-task \\\n  --cluster my-cluster \\\n  --task-definition my-frontend:3 \\\n  --launch-type FARGATE \\\n  --network-configuration \'{\n    "awsvpcConfiguration": {\n      "subnets": ["subnet-abc123"],\n      "securityGroups": ["sg-def456"],\n      "assignPublicIp": "ENABLED"\n    }\n  }\'',
    cdkExample: 'import * as ecs from \'aws-cdk-lib/aws-ecs\';\n\n// Fargate task definition with fine-grained resource control\nconst taskDef = new ecs.FargateTaskDefinition(this, \'TaskDef\', {\n  memoryLimitMiB: 512,\n  cpu: 256,\n});\n\ntaskDef.addContainer(\'web\', {\n  image: ecs.ContainerImage.fromEcrRepository(repo, \'latest\'),\n  portMappings: [{ containerPort: 3000 }],\n  logging: ecs.LogDrivers.awsLogs({ streamPrefix: \'frontend\' }),\n  environment: {\n    NODE_ENV: \'production\',\n    API_URL: \'https://api.example.com\',\n  },\n});',
  },
  {
    id: 'ecr',
    name: 'ECR',
    fullName: 'Elastic Container Registry',
    cat: 'containers',
    level: 'intermediate',
    icon: '\u{1F4CB}',
    short: 'A private Docker image registry. Store, manage, and deploy your container images within AWS.',
    analogy: 'A private app store for your Docker images \u2014 only your team can upload and download from it.',
    detail: 'ECR is like Docker Hub, but private and inside your AWS account. You push Docker images to ECR, and ECS/Fargate/EKS pull from it when deploying containers. It scans images for vulnerabilities, supports lifecycle policies to clean up old images, and integrates with IAM for access control.',
    useCases: [
      'Storing Docker images for CI/CD pipelines',
      'Serving as the image source for ECS/EKS deployments',
    ],
    keyTerms: {
      Repository: 'A collection of Docker images (one per app/service)',
      'Image Tag': 'A version label (like "latest", "v1.2.3")',
    },
    pricing: 'Free tier: 500 MB storage/month for private repos. After: $0.10/GB/month.',
    code: '# Build, tag, and push a Docker image to ECR\n\n# 1. Authenticate Docker to ECR\naws ecr get-login-password --region us-east-1 | \\\n  docker login --username AWS \\\n  --password-stdin 123456789.dkr.ecr.us-east-1.amazonaws.com\n\n# 2. Build your image\ndocker build -t my-frontend-app .\n\n# 3. Tag it for ECR\ndocker tag my-frontend-app:latest \\\n  123456789.dkr.ecr.us-east-1.amazonaws.com/my-frontend-app:latest\n\n# 4. Push it\ndocker push \\\n  123456789.dkr.ecr.us-east-1.amazonaws.com/my-frontend-app:latest',
    howItWorks: 'ECR is a fully managed Docker container registry. Each repository in ECR corresponds to one application or service \u2014 for example, you might have a "frontend" repo for your React app and a "backend" repo for your API. Inside each repository, images are distinguished by tags (like "latest", "v2.1.0", or a Git commit SHA). Under the hood, ECR stores images as layers in S3 and caches them for fast pulls.\n\nWhen ECS, Fargate, or EKS needs to launch a container, it pulls the image from ECR using the URI format: <account-id>.dkr.ecr.<region>.amazonaws.com/<repo-name>:<tag>. Authentication is handled via IAM \u2014 your task execution role needs ecr:GetAuthorizationToken and ecr:BatchGetImage permissions. ECR also supports cross-account and cross-region image replication, which is useful for multi-region deployments.\n\nECR includes two features that frontend teams should enable from day one: image scanning and lifecycle policies. Image scanning (powered by Amazon Inspector or the basic Clair scanner) checks each pushed image for known CVEs in OS packages and language dependencies. Lifecycle policies automatically delete old images based on rules you define (e.g., keep only the last 10 tagged images, delete untagged images older than 7 days), which prevents your storage costs from growing indefinitely.',
    gotchas: [
      'The ECR login token expires every 12 hours. CI/CD pipelines must call "aws ecr get-login-password" at the start of every build, not cache the token.',
      'Pushing to a tag that already exists (like "latest") silently overwrites the previous image. Use immutable tags in production (e.g., Git SHA or semantic version) so you can always roll back to a specific build.',
      'ECR has a default limit of 10,000 images per repository. Without lifecycle policies, long-running CI pipelines will hit this limit and fail on push.',
      'If your ECS tasks pull images across regions (e.g., task in us-west-2, image in us-east-1), you pay data transfer fees and experience slower pulls. Use ECR replication to keep images in the same region as your compute.',
    ],
    whenNotToUse: [
      'Open-source projects that need public image distribution \u2014 ECR Public Gallery exists, but Docker Hub has far better discoverability and community reach.',
      'You are using a multi-cloud strategy and need a registry accessible from GCP/Azure without VPN \u2014 consider a cloud-agnostic registry like GitHub Container Registry or Artifactory.',
    ],
    relatedServices: ['ecs', 'fargate', 'eks', 'codebuild', 'iam'],
    relatedGuides: ['kubernetes'],
    cliExample: '# Create a repo and set a lifecycle policy\naws ecr create-repository \\\n  --repository-name my-frontend-app \\\n  --image-scanning-configuration scanOnPush=true\n\n# Keep only the 20 most recent images\naws ecr put-lifecycle-policy \\\n  --repository-name my-frontend-app \\\n  --lifecycle-policy-text \'{\n    "rules": [{\n      "rulePriority": 1,\n      "description": "Keep last 20 images",\n      "selection": {\n        "tagStatus": "any",\n        "countType": "imageCountMoreThan",\n        "countNumber": 20\n      },\n      "action": { "type": "expire" }\n    }]\n  }\'',
    cdkExample: 'import * as ecr from \'aws-cdk-lib/aws-ecr\';\n\nconst repo = new ecr.Repository(this, \'FrontendRepo\', {\n  repositoryName: \'my-frontend-app\',\n  imageScanOnPush: true,\n  // Auto-clean old images\n  lifecycleRules: [\n    {\n      description: \'Keep last 20 images\',\n      maxImageCount: 20,\n    },\n    {\n      description: \'Delete untagged after 7 days\',\n      tagStatus: ecr.TagStatus.UNTAGGED,\n      maxImageAge: cdk.Duration.days(7),\n    },\n  ],\n});',
  },
  {
    id: 'eks',
    name: 'EKS',
    fullName: 'Elastic Kubernetes Service',
    cat: 'containers',
    level: 'advanced',
    icon: '\u2638\uFE0F',
    short: 'Managed Kubernetes. If your team already uses Kubernetes, EKS runs and manages the control plane for you.',
    analogy: 'An orchestra conductor (Kubernetes) that AWS keeps in top shape \u2014 you bring the musicians (containers).',
    detail: 'EKS runs a managed Kubernetes control plane. If you\'re not already using Kubernetes, you probably don\'t need EKS \u2014 ECS + Fargate is simpler. But if your team has Kubernetes expertise or needs multi-cloud portability, EKS gives you a production-ready K8s cluster without managing the control plane.',
    useCases: [
      'Teams with existing Kubernetes workflows',
      'Multi-cloud strategies',
      'Complex microservice architectures',
    ],
    keyTerms: {
      Cluster: 'A Kubernetes deployment (control plane + worker nodes)',
      Pod: 'The smallest unit in Kubernetes \u2014 one or more containers',
      Node: 'A worker machine that runs pods',
    },
    pricing: '$0.10/hr for the EKS control plane + cost of EC2/Fargate for worker nodes.',
    code: '# Deploy a frontend app to EKS using kubectl\n\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: web\n        image: 123456789.dkr.ecr.us-east-1.amazonaws.com/frontend:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi',
    howItWorks: 'EKS provisions and manages the Kubernetes control plane for you \u2014 the API server, etcd (the cluster state database), the scheduler, and the controller manager. These run across multiple Availability Zones for high availability. Your responsibility is the data plane: the worker nodes where your containers actually run. You have three options for worker nodes: self-managed EC2 instances, EKS Managed Node Groups (AWS handles provisioning and updates), or Fargate (serverless, no nodes to manage at all).\n\nKubernetes introduces its own abstraction layer on top of containers. Instead of task definitions (ECS), you write YAML manifests for Deployments, Services, Ingresses, ConfigMaps, and Secrets. A Deployment ensures a specified number of Pod replicas are running. A Service gives your Pods a stable network endpoint. An Ingress (paired with the AWS Load Balancer Controller) automatically provisions an ALB to route external traffic to your Service. For frontend engineers, this means your React/Next.js app runs in Pods behind an Ingress-managed ALB.\n\nEKS integrates deeply with AWS through add-ons and controllers. The AWS Load Balancer Controller maps Ingress resources to ALBs. The EBS CSI Driver lets Pods mount persistent EBS volumes. IAM Roles for Service Accounts (IRSA) lets each Pod assume a specific IAM role without sharing credentials. ExternalDNS can automatically create Route 53 records for your services. These integrations bridge the gap between Kubernetes\' cloud-agnostic model and AWS-specific features.',
    gotchas: [
      'Kubernetes version upgrades are mandatory \u2014 EKS supports each minor version for about 14 months. Skipping upgrades means your cluster reaches end-of-life and stops receiving security patches. Plan for at least one upgrade per year.',
      'The EKS control plane costs $0.10/hr ($73/month) even when running zero workloads. This is a fixed cost on top of your compute costs, which makes EKS expensive for small projects compared to ECS.',
      'YAML sprawl is real. A simple frontend deployment can require a Deployment, Service, Ingress, ConfigMap, HPA, and PDB \u2014 six files before you even get to CI/CD. Use Helm charts or Kustomize to manage the complexity.',
      'kubectl must match the cluster version (within one minor version). If your local kubectl is v1.28 but EKS is v1.30, mysterious errors occur. Use "aws eks update-kubeconfig" to configure access and keep kubectl updated.',
    ],
    whenNotToUse: [
      'Small teams without Kubernetes experience \u2014 the learning curve is steep and ECS + Fargate achieves the same result with far less operational overhead.',
      'Simple web apps with 1-3 services \u2014 the Kubernetes abstraction layer adds complexity that only pays off at scale (10+ microservices, multiple teams, complex routing).',
      'Tight budgets for non-production environments \u2014 the $73/month control plane fee per cluster adds up quickly across dev, staging, and production.',
    ],
    relatedServices: ['fargate', 'ecr', 'ec2', 'elb', 'iam'],
    relatedGuides: ['eks', 'kubernetes'],
    cliExample: '# Create an EKS cluster with a managed node group\naws eks create-cluster \\\n  --name my-frontend-cluster \\\n  --role-arn arn:aws:iam::123456789:role/EksClusterRole \\\n  --resources-vpc-config \\\n    subnetIds=subnet-abc,subnet-def,securityGroupIds=sg-xyz\n\n# After cluster is ACTIVE (~10 min), add nodes\naws eks create-nodegroup \\\n  --cluster-name my-frontend-cluster \\\n  --nodegroup-name frontend-nodes \\\n  --node-role arn:aws:iam::123456789:role/EksNodeRole \\\n  --subnets subnet-abc subnet-def \\\n  --instance-types t3.medium \\\n  --scaling-config minSize=2,maxSize=5,desiredSize=3',
    cdkExample: 'import * as eks from \'aws-cdk-lib/aws-eks\';\nimport * as ec2 from \'aws-cdk-lib/aws-ec2\';\n\nconst cluster = new eks.Cluster(this, \'FrontendCluster\', {\n  version: eks.KubernetesVersion.V1_30,\n  defaultCapacity: 0, // We\'ll add our own node group\n});\n\n// Add a managed node group for frontend workloads\ncluster.addNodegroupCapacity(\'FrontendNodes\', {\n  instanceTypes: [new ec2.InstanceType(\'t3.medium\')],\n  minSize: 2,\n  maxSize: 5,\n  desiredSize: 3,\n  labels: { role: \'frontend\' },\n});\n\n// Deploy a K8s manifest directly from CDK\ncluster.addManifest(\'FrontendApp\', {\n  apiVersion: \'apps/v1\',\n  kind: \'Deployment\',\n  metadata: { name: \'frontend\' },\n  spec: {\n    replicas: 3,\n    selector: { matchLabels: { app: \'frontend\' } },\n    template: {\n      metadata: { labels: { app: \'frontend\' } },\n      spec: {\n        containers: [{\n          name: \'web\',\n          image: \'my-frontend:latest\',\n          ports: [{ containerPort: 80 }],\n        }],\n      },\n    },\n  },\n});',
  },
]
